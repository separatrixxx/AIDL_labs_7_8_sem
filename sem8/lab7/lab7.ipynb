{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 7, Лохматов Никита Игоревич М8О-406Б-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Выбор начальных условий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Набор данных для семантической сегментации: product-masks-sample \n",
    "\n",
    "Этот набор данных включает в себя изображения, созданные на основе 3D-моделей предметов домашнего интерьера и гостиной обстановки.\n",
    "\n",
    "Особенностью данного датасета является фотореалистичное исполнение изображений, что обеспечивает высокую степень достоверности при использовании их для обучения компьютерных моделей. Благодаря этому, результаты сегментации получаются максимально приближенными к работе с реальными фотографиями.\n",
    "\n",
    "Подобные данные представляют особую ценность для разработки систем автоматизированного учета и инвентаризации, где требуется точное распознавание объектов через камеры видеонаблюдения или мобильные устройства. Возможность работы с синтетическими, но при этом реалистичными изображениями значительно упрощает процесс обучения моделей, позволяя избежать трудоемкого сбора и разметки реальных фотографий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Выбор метрик качества и обоснование\n",
    "\n",
    "Для задачи семантической сегментации ключевым аспектом является выбор метрик, которые корректно отражают качество предсказаний с учетом специфики задачи. Здесь важно учитывать многоклассовую классификацию, пространственную структуру объектов и возможный дисбаланс классов.\n",
    "\n",
    "В качестве основной метрики была выбрана IoU (Intersection over Union) — коэффициент, вычисляемый как отношение площади пересечения предсказанной и истинной масок к площади их объединения для каждого класса. Эта метрика хорошо отражает точность локализации объектов и учитывает их форму, что особенно важно в семантической сегментации.\n",
    "\n",
    "В отличие от задач классификации, стандартная Accuracy здесь часто оказывается неинформативной. Это связано с тем, что изображения могут содержать доминирующие классы (например, фон), которые занимают большую часть сцены. В таком случае модель может достигать высокой Accuracy, просто предсказывая везде фоновый класс, при этом плохо распознавая остальные объекты. По этой причине Accuracy не была использована для оценки модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Создание бейзлайна и оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import torchinfo\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations\n",
    "import os\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Увеличиваем таймауты для загрузки датасета\n",
    "os.environ[\"HF_DATASETS_TIMEOUT\"] = \"600\"\n",
    "\n",
    "# Скачиваем датасет\n",
    "dataset = datasets.load_dataset('Nfiniteai/product-masks-sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем бейзлайн. Нормализуем название классов и уберём из тренировочной выборки те записи, в которых есть классы, не предусмотренные в валидационной выборке\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c5033c3b854c589c243a6e0a8360a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0e41ff29a04b7e9eb056ec593c6dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Делим датасет на валидационный и тренировочный\n",
    "val_dataset = dataset['val']\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Обработка валидационного набора\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda batch: {\n",
    "        'category': [x.lower().replace(' ', '_') for x in batch['category']]\n",
    "    },\n",
    "    batched=True,\n",
    "    batch_size=300,\n",
    "    writer_batch_size=300,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "\n",
    "# Обработка тренировочного набора\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda batch: {\n",
    "        'category': [x.lower().replace(' ', '_') for x in batch['category']]\n",
    "    },\n",
    "    batched=True,\n",
    "    batch_size=300,\n",
    "    writer_batch_size=300,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "\n",
    "# Фильтруем датасет\n",
    "train_dataset = train_dataset.remove_columns(set(train_dataset.features) - {\n",
    "    'image',\n",
    "    'bbox',\n",
    "    'category',\n",
    "    'mask'\n",
    "})\n",
    "\n",
    "val_dataset = val_dataset.remove_columns(set(train_dataset.features) - {\n",
    "    'image',\n",
    "    'bbox',\n",
    "    'category',\n",
    "    'mask',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для корректного обучения модели сначала исключим из тренировочных данных классы, отсутствующие в валидационной выборке, и проверим, что не удалили слишком много примеров. Затем каждому классу назначим уникальный числовой ID, добавив специальный идентификатор <back> для фона.\n",
    "\n",
    "Отдельное внимание уделим обработке масок: функция transform_mask конвертирует черно-белую маску в матрицу числовых идентификаторов классов, где фон помечается ID класса <back>. Это обеспечит корректную подготовку данных для обучения модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66e7587fb5243da845f73fe68000b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_class = '<back>'\n",
    "\n",
    "categories = set(val_dataset['category'])\n",
    "categories = {category: index for index, category in enumerate(categories)}\n",
    "categories[new_class] = len(categories)\n",
    "\n",
    "train_dataset = train_dataset.filter(\n",
    "    lambda category: category in categories,\n",
    "    load_from_cache_file=False,\n",
    "    writer_batch_size=300,\n",
    "    input_columns=['category'],\n",
    ")\n",
    "\n",
    "transform_image = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "def transform_mask(mask, class_index, *, threshold=0.5, classes_amount=1):\n",
    "    mask = mask.resize((224, 224))\n",
    "    mask_np = np.array(mask).astype(np.float32) / 255.0\n",
    "\n",
    "    object_mask = (mask_np >= threshold).astype(int) * class_index\n",
    "    background_mask = (mask_np < threshold).astype(int) * categories[new_class]\n",
    "    total_mask = object_mask + background_mask\n",
    "\n",
    "    return torch.from_numpy(total_mask)\n",
    "\n",
    "def default_transform(x, bbox, y):\n",
    "    return x, y\n",
    "\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, categories, *, max_len=None, transform=None, threshold=0.5):\n",
    "        self._dataset = dataset\n",
    "        self._categories = categories\n",
    "        self._max_len = float('inf') if max_len is None else max_len\n",
    "        self._threshold = threshold\n",
    "        self._transform = transform or default_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self._dataset), self._max_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self._dataset[idx]\n",
    "        image, mask = example['image'].convert('RGB'), example['mask']\n",
    "        class_index = self._categories[example['category']]\n",
    "        bbox = example['bbox']\n",
    "\n",
    "        image, mask = self._transform(image, bbox, mask)\n",
    "\n",
    "        image = transform_image(image)\n",
    "        mask = transform_mask(\n",
    "            mask,\n",
    "            class_index,\n",
    "            threshold=self._threshold,\n",
    "            classes_amount=len(self._categories),\n",
    "        )\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, для визуальной проверки работоспособности обученной модели напишем функцию test_model, которая визуально отображает маску, предсказанную моделью, и ожидаемую маску для класса category_index, объект которого находится на целевом изображении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_metrics(outputs, masks, metrics):\n",
    "    predicted_classes = outputs.argmax(dim=1)\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(\n",
    "        predicted_classes, masks,\n",
    "        mode='multiclass',\n",
    "        num_classes=len(categories) - 1,\n",
    "        ignore_index=categories[new_class],\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        metric_name: metric_function(tp, fp, fn, tn).item()\n",
    "        for metric_name, metric_function in metrics.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(model, dataset, loss, optimizer, *, num_epochs=1):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_size = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for step_number, (images, masks) in enumerate(dataset, 1):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss_value = loss(outputs, masks)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_size = images.size()[0]\n",
    "            total_loss += loss_value.item() * batch_size\n",
    "            total_size += batch_size\n",
    "\n",
    "            yield {\n",
    "                'epoch': epoch,\n",
    "                'step': step_number,\n",
    "                'loss': total_loss / total_size,\n",
    "            }\n",
    "\n",
    "\n",
    "def eval_model(model, ds, metrics):\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in ds:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            all_labels.append(masks.cpu())\n",
    "            all_preds.append(outputs.cpu())\n",
    "\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    all_preds  = torch.cat(all_preds, dim=0)\n",
    "\n",
    "    return _calculate_metrics(all_preds, all_labels, metrics)\n",
    "\n",
    "def test_model(model, image, mask, category_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        C, W, H = image.shape\n",
    "        image = image.resize(1, C, W, H)\n",
    "        image = image.to(device)\n",
    "        preds = model(image)[0].cpu()\n",
    "\n",
    "    predicted_mask = torch.argmax(preds, dim=0).numpy()\n",
    "    predicted_mask = (predicted_mask == category_index).astype(np.uint8) * 255\n",
    "\n",
    "    predicted_mask_image = Image.fromarray(predicted_mask, mode='L')\n",
    "    mask_image = Image.fromarray((mask.numpy() == category_index).astype(np.uint8) * 255, mode='L')\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(mask_image)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(predicted_mask_image)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Задаём метрики\n",
    "metrics = {\n",
    "    'iou': lambda tp, fp, fn, tn: smp.metrics.iou_score(tp, fp, fn, tn, reduction='micro'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 3.4138243198394775\n",
      "Loss: 2.9628832123496314\n",
      "Loss: 3.218384481611706\n",
      "Loss: 2.97334647563196\n",
      "Loss: 2.951845012060026\n",
      "Loss: 2.8129118564082125\n",
      "Loss: 2.8080450042349394\n",
      "Loss: 2.764828304169883\n",
      "Loss: 2.843307321454272\n",
      "Loss: 2.8291472576476715\n",
      "Loss: 2.8225717119651264\n",
      "Loss: 2.7533656970874683\n",
      "Loss: 2.7264289796844987\n",
      "Epoch: 2\n",
      "Loss: 1.8158044815063477\n",
      "Loss: 1.9192100167274475\n",
      "Loss: 2.383252271584102\n",
      "Loss: 2.2137862751560826\n",
      "Loss: 2.2687896693625103\n",
      "Loss: 2.180067342870376\n",
      "Loss: 2.2766229168313448\n",
      "Loss: 2.2886755751891874\n",
      "Loss: 2.419335310841784\n",
      "Loss: 2.4616209949765886\n",
      "Loss: 2.4821709783950654\n",
      "Loss: 2.455619841008573\n",
      "Loss: 2.4502612362223224\n",
      "Epoch: 3\n",
      "Loss: 1.7126054763793945\n",
      "Loss: 1.8325958035208962\n",
      "Loss: 2.4014258952367875\n",
      "Loss: 2.334663268058531\n",
      "Loss: 2.3277903242809015\n",
      "Loss: 2.243458205578374\n",
      "Loss: 2.324028146071512\n",
      "Loss: 2.3340043601855425\n",
      "Loss: 2.444059279229906\n",
      "Loss: 2.4913194192634833\n",
      "Loss: 2.504410711845549\n",
      "Loss: 2.4632310878049144\n",
      "Loss: 2.4410962624983354\n"
     ]
    }
   ],
   "source": [
    "model_cnn = smp.Unet(\n",
    "    encoder_name='resnet18',\n",
    "    encoder_weights='imagenet',\n",
    "    in_channels=3,\n",
    "    classes=len(categories),\n",
    ")\n",
    "\n",
    "train_ds = SegmentationDataset(train_dataset, categories)\n",
    "val_ds = SegmentationDataset(val_dataset, categories)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "model_cnn = model_cnn.to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "\n",
    "    train_logs = train_model(\n",
    "        model_cnn,\n",
    "        train_loader,\n",
    "        torch.nn.CrossEntropyLoss(ignore_index=categories[new_class]),\n",
    "        torch.optim.Adam(model_cnn.parameters(), lr=1e-3),\n",
    "    )\n",
    "\n",
    "    for i, log in enumerate(train_logs):\n",
    "        if i % 10 == 0:\n",
    "            print(f'Loss: {log[\"loss\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение трансформерной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 3.9942052364349365\n",
      "Loss: 2.9918160980398003\n",
      "Loss: 3.2698717628206526\n",
      "Loss: 3.073573416279208\n",
      "Loss: 3.0041509168904\n",
      "Loss: 2.846421241760254\n",
      "Loss: 2.860645837471133\n",
      "Loss: 2.8707947059416434\n",
      "Loss: 2.9498332731517745\n",
      "Loss: 2.9553522572412594\n",
      "Loss: 2.957405154657836\n",
      "Loss: 2.891308440281464\n",
      "Loss: 2.8535280203031115\n",
      "Epoch: 2\n",
      "Loss: 1.3732373714447021\n",
      "Loss: 2.0182564854621887\n",
      "Loss: 2.5124746759732566\n",
      "Loss: 2.3883527921092127\n",
      "Loss: 2.3510602087509342\n",
      "Loss: 2.2407586352497924\n",
      "Loss: 2.350883430144826\n",
      "Loss: 2.3755307357076187\n",
      "Loss: 2.474949660860462\n",
      "Loss: 2.5039859479600257\n",
      "Loss: 2.5218013259443905\n",
      "Loss: 2.4774803100405514\n",
      "Loss: 2.453173875808716\n",
      "Epoch: 3\n",
      "Loss: 1.3790245056152344\n",
      "Loss: 1.768868793140758\n",
      "Loss: 2.2981909854071483\n",
      "Loss: 2.165590968824202\n",
      "Loss: 2.1461121527160087\n",
      "Loss: 2.0417541651164783\n",
      "Loss: 2.138639597619166\n",
      "Loss: 2.1959176021562494\n",
      "Loss: 2.315572620174031\n",
      "Loss: 2.3463865289321313\n",
      "Loss: 2.401363742823648\n",
      "Loss: 2.393601336457708\n",
      "Loss: 2.3880639494943225\n"
     ]
    }
   ],
   "source": [
    "model_transform = smp.Segformer(\n",
    "    encoder_name='mit_b0',\n",
    "    encoder_weights='imagenet',\n",
    "    in_channels=3,\n",
    "    classes=len(categories),\n",
    ")\n",
    "\n",
    "torchinfo.summary(model_transform)\n",
    "\n",
    "train_ds = SegmentationDataset(train_dataset, categories)\n",
    "val_ds = SegmentationDataset(val_dataset, categories)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "model_transform = model_transform.to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "\n",
    "    train_logs = train_model(\n",
    "        model_transform,\n",
    "        train_loader,\n",
    "        torch.nn.CrossEntropyLoss(ignore_index=categories[new_class]),\n",
    "        torch.optim.Adam(model_transform.parameters(), lr=1e-3),\n",
    "    )\n",
    "\n",
    "    for i, log in enumerate(train_logs):\n",
    "        if i % 10 == 0:\n",
    "         print(f'Loss: {log[\"loss\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Оценка качества моделей по выбранным метрикам на выбранном наборе данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN res: iou:0.0591\n"
     ]
    }
   ],
   "source": [
    "results_cnn = eval_model(model_cnn, val_loader, metrics)\n",
    "\n",
    "print('CNN res: {}'.format(\n",
    "    ', '.join(f\"{name}:{value:.4f}\" for name, value in results_cnn.items())\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформерная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform res: iou:0.1348\n"
     ]
    }
   ],
   "source": [
    "results_transform = eval_model(model_transform, val_loader, metrics)\n",
    "\n",
    "print('Transform res: {}'.format(\n",
    "    ', '.join(f\"{name}:{value:.4f}\" for name, value in results_transform.items())\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Улучшение бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Сформулировать гипотезы (аугментации данных, подбор моделей, подбор гиперпараметров и т.д.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Обрезка изображений\n",
    "\n",
    "В текущем бейзлайне основную часть изображения занимает задний фон, и доля пикселей объекта становится слишком малой. Чтобы повысить информативность данных, исключим фон, обрезав изображения и маски по bounding box — так на кадре останется только целевой объект. Это увеличит относительную долю полезных пикселей и, вероятно, улучшит обучение модели. Для реализации создадим функцию crop_image, выполняющую обрезку по bbox, и интегрируем её в датасеты.\n",
    "\n",
    "2. Аугментация\n",
    "\n",
    "Вторая гипотеза — применение аугментаций для повышения обобщающей способности модели. Для этого с помощью albumentations реализуем пайплайн _augment, включающий случайное горизонтальное отражение и вращение до 15°. Благодаря albumentations, одинаковые преобразования будут применяться как к изображению, так и к маске."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Проверка гипотез"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Обрезка изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, bbox, mask):\n",
    "    x, y, w, h = bbox\n",
    "    cropped_image = image.crop((x, y, x + w, y + h))\n",
    "    cropped_mask = mask.crop((x, y, x + w, y + h))\n",
    "    return cropped_image, cropped_mask\n",
    "\n",
    "\n",
    "train_ds = SegmentationDataset(train_dataset, categories, transform=crop_image)\n",
    "val_ds = SegmentationDataset(val_dataset, categories, transform=crop_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "CNN crop res: iou:0.0080\n",
      "Epoch: 2\n",
      "CNN crop res: iou:0.0080\n",
      "Epoch: 3\n",
      "CNN crop res: iou:0.0080\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name='resnet18',\n",
    "    encoder_weights='imagenet',\n",
    "    in_channels=3,\n",
    "    classes=len(categories),\n",
    ").to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "\n",
    "    train_logs = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        torch.nn.CrossEntropyLoss(ignore_index=categories[new_class]),\n",
    "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "    )\n",
    "\n",
    "    results = eval_model(model, val_loader, metrics)\n",
    "\n",
    "    print('CNN crop res: {}'.format(\n",
    "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Transform crop res: iou:0.0210\n",
      "Epoch: 2\n",
      "Transform crop res: iou:0.0210\n",
      "Epoch: 3\n",
      "Transform crop res: iou:0.0210\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "model = smp.Segformer(\n",
    "    encoder_name='mit_b0',\n",
    "    encoder_weights='imagenet',\n",
    "    in_channels=3,\n",
    "    classes=len(categories),\n",
    ").to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "\n",
    "    train_logs = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        torch.nn.CrossEntropyLoss(ignore_index=categories[new_class]),\n",
    "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "    )\n",
    "\n",
    "    results = eval_model(model, val_loader, metrics)\n",
    "    \n",
    "    print('Transform crop res: {}'.format(\n",
    "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Аугментация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "_augment = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.Rotate(limit=15, p=0.5),\n",
    "    ],\n",
    "    additional_targets={'mask': 'mask'},\n",
    ")\n",
    "\n",
    "\n",
    "def augment_pair(image, mask):\n",
    "    image = np.array(image)\n",
    "    mask = np.array(mask)\n",
    "\n",
    "    augmented = _augment(image=image, mask=mask)\n",
    "    return Image.fromarray(augmented['image']), Image.fromarray(augmented['mask'])\n",
    "\n",
    "\n",
    "def transform(image, bbox, mask):\n",
    "    image, mask = augment_pair(image, mask)\n",
    "    return crop_image(image, bbox, mask)\n",
    "\n",
    "\n",
    "train_ds = SegmentationDataset(train_dataset, categories, transform=transform)\n",
    "val_ds = SegmentationDataset(val_dataset, categories, transform=crop_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "CNN aug res: iou:0.0131\n",
      "Epoch: 2\n",
      "CNN aug res: iou:0.0131\n",
      "Epoch: 3\n",
      "CNN aug res: iou:0.0131\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name='resnet18',\n",
    "    encoder_weights='imagenet',\n",
    "    in_channels=3,\n",
    "    classes=len(categories),\n",
    ").to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "\n",
    "    train_logs = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        torch.nn.CrossEntropyLoss(ignore_index=categories[new_class]),\n",
    "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "    )\n",
    "\n",
    "    results = eval_model(model, val_loader, metrics)\n",
    "\n",
    "    print('CNN aug res: {}'.format(\n",
    "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "transform aug res: iou:0.0152\n",
      "Epoch: 2\n",
      "transform aug res: iou:0.0152\n",
      "Epoch: 3\n",
      "transform aug res: iou:0.0152\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "model = smp.Segformer(\n",
    "    encoder_name='mit_b0',\n",
    "    encoder_weights='imagenet',\n",
    "    in_channels=3,\n",
    "    classes=len(categories),\n",
    ").to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "\n",
    "    train_logs = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        torch.nn.CrossEntropyLoss(ignore_index=categories[new_class]),\n",
    "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "    )\n",
    "\n",
    "    results = eval_model(model, val_loader, metrics)\n",
    "\n",
    "    print('transform aug res: {}'.format(\n",
    "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По результатам обучения базовых моделей для задачи семантической сегментации можно отметить следующее. На второй эпохе наблюдается просадка IoU, вероятно связанная с адаптацией архитектур к задаче. SegFormer на MiT-B0 обеспечил более высокое качество сегментации и стабильное обучение по сравнению с U-Net на ResNet18.\n",
    "\n",
    "Обрезка изображений по bounding box и аугментации улучшили результаты лишь для U-Net, тогда как у SegFormer качество, наоборот, слегка снизилось. Таким образом, предложенные методы усиления бейзлайна эффективны преимущественно для сверточных моделей, в то время как на трансформерной архитектуре SegFormer значимого улучшения не наблюдается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Имплементация алгоритма машинного обучения "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Самостоятельная имплементация алгоритмов машинного обучения для классификации и регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Свёрточная модель**\n",
    "\n",
    "Перейдём к реализации сверточной модели для семантической сегментации — U-Net. В этот раз архитектура оформлена в виде класса UNetImplementation с гибкой конфигурацией уровней через словари, что позволяет легко адаптировать модель под разные задачи и ресурсы.\n",
    "\n",
    "В конструктор передаются параметры каналов для каждого уровня (in_channels, out_channels) и число классов (num_classes) для финального слоя.\n",
    "Модель состоит из энкодера (экстракция признаков с понижением масштаба), центрального боттлнека (максимальное сжатие и увеличение глубины), декодера (апсемплирование и skip-соединения) и финального свёрточного слоя, возвращающего маску предсказаний.\n",
    "Метод forward реализует пошаговую обработку данных: формирование признаков в энкодере, skip-соединения, объединение признаков при декодировании и получение итоговой маски.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetImplementation(torch.nn.Module):\n",
    "    def __init__(self, layers_config, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self._encoder_blocks = torch.nn.ModuleList()\n",
    "        self._downsampling_layers = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in layers_config:\n",
    "            self._encoder_blocks.append(\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(\n",
    "                        layer['in_channels'],\n",
    "                        layer['out_channels'],\n",
    "                        kernel_size=3,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Conv2d(\n",
    "                        layer['out_channels'],\n",
    "                        layer['out_channels'],\n",
    "                        kernel_size=3,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    torch.nn.ReLU(),\n",
    "                )\n",
    "            )\n",
    "            self._downsampling_layers.append(torch.nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        bottleneck_in = layers_config[-1]['out_channels']\n",
    "        bottleneck_out = bottleneck_in * 2\n",
    "\n",
    "        self._bottleneck_block = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(\n",
    "                bottleneck_in,\n",
    "                bottleneck_out,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(\n",
    "                bottleneck_out,\n",
    "                bottleneck_out,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self._upsampling_layers = torch.nn.ModuleList()\n",
    "        self._decoder_blocks = torch.nn.ModuleList()\n",
    "        reversed_config = list(reversed(layers_config))\n",
    "\n",
    "        for layer in reversed_config:\n",
    "            self._upsampling_layers.append(\n",
    "                torch.nn.ConvTranspose2d(\n",
    "                    bottleneck_out,\n",
    "                    layer['out_channels'],\n",
    "                    kernel_size=2,\n",
    "                    stride=2,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            self._decoder_blocks.append(\n",
    "                torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(\n",
    "                        layer['out_channels'] * 2,\n",
    "                        layer['out_channels'],\n",
    "                        kernel_size=3,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Conv2d(\n",
    "                        layer['out_channels'],\n",
    "                        layer['out_channels'],\n",
    "                        kernel_size=3,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    torch.nn.ReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            bottleneck_out = layer['out_channels']\n",
    "\n",
    "        self._final_convolution = torch.nn.Conv2d(\n",
    "            layers_config[0]['out_channels'],\n",
    "            num_classes,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        encoder_feature_maps = []\n",
    "        x = input_tensor\n",
    "\n",
    "        for encoder_block, downsample in zip(self._encoder_blocks, self._downsampling_layers):\n",
    "            x = encoder_block(x)\n",
    "            encoder_feature_maps.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self._bottleneck_block(x)\n",
    "\n",
    "        for index in range(len(self._upsampling_layers)):\n",
    "            x = self._upsampling_layers[index](x)\n",
    "            skip_connection = encoder_feature_maps[-(index + 1)]\n",
    "            x = self._decoder_blocks[index](torch.cat([x, skip_connection], dim=1))\n",
    "\n",
    "        return self._final_convolution(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Трансформерная модель**\n",
    "\n",
    "Перейдём к собственной реализации трансформерной модели для семантической сегментации, оформленной в виде классов. Архитектура включает ключевые компоненты:\n",
    "\n",
    "- PatchEmbedding — разбивает изображение на патчи и проецирует их в скрытое пространство через свёртку;\n",
    "- TransformerEncoderBlock — состоит из слоёв нормализации, multi-head attention и двухслойного MLP с GELU и дропаутом, с резидуальными связями для стабильности обучения;\n",
    "- TransformerSegmentationModel — объединяет все модули в единую модель сегментации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(torch.nn.Module):\n",
    "    def __init__(self, in_channels=3, patch_size=16, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = torch.nn.Conv2d(\n",
    "            in_channels,\n",
    "            emb_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(torch.nn.Module):\n",
    "    def __init__(self, emb_dim=256, num_heads=8, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norms = torch.nn.ModuleList([\n",
    "            torch.nn.LayerNorm(emb_dim),\n",
    "            torch.nn.LayerNorm(emb_dim)\n",
    "        ])\n",
    "\n",
    "        self.attention = torch.nn.MultiheadAttention(\n",
    "            emb_dim,\n",
    "            num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        hidden_dim = int(emb_dim * mlp_ratio)\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, hidden_dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(hidden_dim, emb_dim),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layer_norms[0](x), x, x)[0]\n",
    "        x = x + self.mlp(self.layer_norms[1](x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerSegmentationModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        num_classes=1,\n",
    "        patch_size=16,\n",
    "        emb_dim=256,\n",
    "        depth=6,\n",
    "        num_heads=8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(in_channels, patch_size, emb_dim)\n",
    "\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "            TransformerEncoderBlock(emb_dim, num_heads)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        num_patches = (224 // patch_size) ** 2\n",
    "        self.position_embedding = torch.nn.Parameter(\n",
    "            torch.zeros(1, num_patches, emb_dim)\n",
    "        )\n",
    "\n",
    "        self.reconstruction = torch.nn.ConvTranspose2d(\n",
    "            emb_dim,\n",
    "            num_classes,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.position_embedding\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        spatial_dim = int((x.size(1)) ** 0.5)\n",
    "        x = x.transpose(1, 2).reshape(batch_size, -1, spatial_dim, spatial_dim)\n",
    "        x = self.reconstruction(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Обучение имплементированной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свёрточная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Res: iou:0.0002\n",
      "Epoch: 2\n",
      "Res: iou:0.0002\n",
      "Epoch: 3\n",
      "Res: iou:0.0002\n"
     ]
    }
   ],
   "source": [
    "train_ds = SegmentationDataset(train_dataset, categories)\n",
    "val_ds = SegmentationDataset(val_dataset, categories)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "config = [\n",
    "    {'in_channels': 3, 'out_channels': 64},\n",
    "    {'in_channels': 64, 'out_channels': 128},\n",
    "]\n",
    "model = UNetImplementation(config, num_classes=len(categories)).to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "\n",
    "    train_logs = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        torch.nn.CrossEntropyLoss(ignore_index=categories[new_class]),\n",
    "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "    )\n",
    "\n",
    "    results = eval_model(model, val_loader, metrics)\n",
    "    \n",
    "    print('Res: {}'.format(\n",
    "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформерная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Res: iou:0.0128\n",
      "Epoch: 2\n",
      "Res: iou:0.0128\n",
      "Epoch: 3\n",
      "Res: iou:0.0128\n"
     ]
    }
   ],
   "source": [
    "train_ds = SegmentationDataset(train_dataset, categories)\n",
    "val_ds = SegmentationDataset(val_dataset, categories)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "model = TransformerSegmentationModel(num_classes=len(categories)).to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "\n",
    "    train_logs = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        torch.nn.CrossEntropyLoss(ignore_index=categories[new_class]),\n",
    "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "    )\n",
    "\n",
    "    results = eval_model(model, val_loader, metrics)\n",
    "\n",
    "    print('Res: {}'.format(\n",
    "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g. Обучение на улучшенном бейзлайне"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свёрточная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Res: iou:0.0091\n",
      "Epoch: 2\n",
      "Res: iou:0.0091\n",
      "Epoch: 3\n",
      "Res: iou:0.0091\n"
     ]
    }
   ],
   "source": [
    "train_ds = SegmentationDataset(train_dataset, categories, transform=transform)\n",
    "val_ds = SegmentationDataset(val_dataset, categories, transform=crop_image)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "model = UNetImplementation(config, num_classes=len(categories)).to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "    \n",
    "    train_logs = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        torch.nn.CrossEntropyLoss(ignore_index=categories[new_class]),\n",
    "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "    )\n",
    "\n",
    "    results = eval_model(model, val_loader, metrics)\n",
    "\n",
    "    print('Res: {}'.format(\n",
    "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформерная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Res: iou:0.0121\n",
      "Epoch: 2\n",
      "Res: iou:0.0121\n",
      "Epoch: 3\n",
      "Res: iou:0.0121\n"
     ]
    }
   ],
   "source": [
    "train_ds = SegmentationDataset(train_dataset, categories, transform=transform)\n",
    "val_ds = SegmentationDataset(val_dataset, categories, transform=crop_image)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "model = TransformerSegmentationModel(num_classes=len(categories)).to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "\n",
    "    train_logs = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        torch.nn.CrossEntropyLoss(ignore_index=categories[new_class]),\n",
    "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "    )\n",
    "\n",
    "    results = eval_model(model, val_loader, metrics)\n",
    "\n",
    "    print('Res: {}'.format(\n",
    "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### j. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собственная трансформерная модель показала худшие результаты на бейзлайне по сравнению с U-Net, а улучшение бейзлайна существенного эффекта не дало.\n",
    "В\n",
    "озможные причины — отсутствие предобучения и меньшая сложность архитектуры по сравнению с U-Net и SegFormer, обученными на больших датасетах.\n",
    "\n",
    "Также собственная реализация уступила по качеству предобученному SegFormer, вероятно, из-за упрощённой структуры и обучения с нуля.\n",
    "\n",
    "Однако применение улучшенного бейзлайна в этом случае дало заметный прирост метрик по сравнению с обычным бейзлайном."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
